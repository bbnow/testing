{"name":"Surf","tagline":"IoT/M2M for Amazon AWS","body":"#Examples\r\n* Example 1 - Simple Setup\r\n* Example 2 - Page Count \r\n\r\n***\r\n\r\n##Example 1 – Simple Set-up\r\nThis example explores connectivity between on-premise data sources and cloud-based Kinesis applications.\r\n\r\n![Example1 Picture](images/surf2_4.jpg)\r\n\r\nIn this example the data source and the application are very simple – data source generates a stream of time-stamps, the application displays the time-stamps on a console.\r\n\r\n###Surf Data Source\r\nThe code for the data source can be found in file `com.informatica.surf.sources.dummy.DummySource.java` [here](orp/Surf/blob/master/sources/dummy/src/main/java/com/informatica/surf/sources/dummy/DummySource.java). \r\n\r\nFor this example the data source generates time-stamps and streams them to Kinesis application. Here is the code snippet performing this operation:\r\n    \r\n    public void read(VDSEventList readEvents) throws Exception {\r\n         Thread.sleep(1000);\r\n         Date d = new Date();\r\n         byte []b = d.toString().getBytes();\r\n         readEvents.addEvent(b, b.length, _headers);\r\n    } \r\n\r\n> Generally, a Surf user does not need to develop data source code. They only need to configure a data source like File-tailer, HTTP, or MQTT. However, the user may also easily develop a new data source as needed.\r\n\r\n###Surf Application\r\nSource code for a simple Surf application used in this example is located in `com.informatica.surf.sample.DumpStream.java` [here](https://github.com/InformaticaCorp/Surf/blob/master/sample/src/main/java/com/informatica/surf/sample/DumpStream.java).\r\n\r\nIn this case the application logic is very simple – take a record from Kinesis stream and print it. Here the code snippet:\r\n\r\n    public void onEvent(KinesisEvent kinesisEvent, long l, boolean b) throws Exception {    \r\n        System.out.println(String.format(\"Received : %s\", kinesisEvent.getData()));\r\n    }\r\n\r\nApplication developer only needs to provide a handler - _onEvent_ – and include the application logic within this handler.\r\n\r\n###Configuration Steps\r\n\r\nPlease, follow the following steps to configure the system:<br>\r\n1. Enable Kinesis Stream<br>\r\n2. Configure and start Surf application<br>\r\n3. Configure and start Surf data source<br>\r\n4. Observe system in action<br>\r\n\r\n1 1. Enable Kinesis Stream\r\n1 2. Configure and start Surf application\r\n1 3. Configure and start Surf data source\r\n1 4. Observe system in action\r\n\r\n**Enabling Kinesis Stream**<br>\r\nFirst we need to create a Kinesis stream. Do this by logging into AWS Kinesis console.\r\n\r\n![Example1 Picture](images/surf2_4.jpg)\r\n\r\nRemember Kinesis stream name as it is used for configuration of Surf processes.\r\n\r\n**Configuring Surf Application**<br>\r\nBring up an EC2 instance and copy Surf distribution package to it. We will refer to the top-level directory of the distribution as _**surf-home**_.\r\nThe distribution package can be found in _assembly/target/surf-1.0-dist_ directory of the Surf tree once it is built by Maven.\r\n\r\nThe next step is to create a configuration file that contain AWS credentials and Kinesis stream name. The file should reside in _**surf-home**/conf_ directory. \r\nLet’s name the file _app-node1.config_ and place following entries into it:\r\n\r\n    aws-access-key-id: <your aws access key>\r\n    aws-secret-key: <your aws secret key>\r\n    aws-kinesis-stream-name: <kinesis stream name>\r\n\r\nNext, make sure that Java 7 JRE is available and is configured properly. Note that note only the right JRE should be available, but also JAVA_HOME should be pointing to the JRE’s directory.  \r\n\r\nNow, the application is ready to be started with the following command line:\r\n\r\n    ./surf.sh dump-stream app-node1\r\n\r\nWhile no output is yet displayed by the application, the application log file can be found in _**surf_home**/log_ directory. The file will contain information about application start up and current listening state.\r\n\r\n**Configuring Surf Data Source**<br>\r\nCopy the surf distribution package to your local machine and ensure Java 7 availability.\r\nSimilarly to application, let’s create a configuration file source_node1.conf in _**surf-home**/conf_ directory containing the following entries: \r\n\r\n    aws-access-key-id: <your aws access key>\r\n    aws-secret-key: <your aws secret key>\r\n    aws-kinesis-stream-name: <kinesis stream name>\r\n    vds-source-class: com.informatica.surf.sources.dummy.DummySource\r\n\r\nNow we are ready to start the data source Surf process with the following command in _**surf-home**/bin_ directory:\r\n\r\n    ./surf.sh start-node source-node1\r\n\r\nAs the process starts, we can examine the log file found in _**surf-home**/log_ directory.\r\n\r\n**Observing System in Action**<br>\r\nAs the data source comes on-line, the Surf application comes to life. You can see the stream of time-stamps appearing on the application console. \r\n\r\n![Example1 Picture](images/surf2_4.jpg)\r\n\r\nYou can now extend the sample in several ways:\r\n* Start another data source process and observe the number of messages flowing through the system increase\r\n* Extend the data source (DummySource.java) to insert another field in the record that would identify it (process id or a UUID) and observe the identifier displayed on the application output console   \r\n\r\n<a name=\"example_2\">\r\n##Example 2 – Page Counts\r\nIn this example we explore more interesting data source and application logic. The example illustrates how Surf can monitor a weblog file and stream newly added records to a Kinesis application.\r\n\r\n![Example1 Picture](images/surf2_4.jpg)\r\n\r\nThe log file in this example contains information about page hits. The sample file can be obtained from this website - [http://ita.ee.lbl.gov/html/traces.html](http://ita.ee.lbl.gov/html/traces.html). We took _epa-http.txt_ file for testing.\r\n\r\nRecords in the file look as follows:\r\n\r\n    query2.lycos.cs.cmu.edu [29:23:53:36] \"GET /Consumer.html HTTP/1.0\" 200 1325\r\n    tanuki.twics.com [29:23:53:53] \"GET /News.html HTTP/1.0\" 200 1014\r\n    wpbfl2-45.gate.net [29:23:54:15] \"GET / HTTP/1.0\" 200 4889\r\n    wpbfl2-45.gate.net [29:23:54:16] \"GET /icons/circle_logo_small.gif HTTP/1.0\" 200 2624\r\n    wpbfl2-45.gate.net [29:23:54:18] \"GET /logos/small_gopher.gif HTTP/1.0\" 200 935\r\n\r\nSurf application logic keeps count of how many times each particular page has been referenced. \r\n\r\n###Configuring Surf Application\r\nSource code for the application can be found in files `com.informatica.surf.sample.PageCount.java` [here](https://github.com/InformaticaCorp/Surf/blob/master/sample/src/main/java/com/informatica/surf/sample/PageCount.java) and `com.informatica.surf.sample.PageCountHandler.java` [here](https://github.com/InformaticaCorp/Surf/blob/master/sample/src/main/java/com/informatica/surf/sample/PageCountHandler.java). Just like in the simple example before, all application logic resides in _onEvent_ handler. The logic deals with taking records off the stream, parsing them to extract the page URL, and then keeping count of how many times each URL has been referenced:   \r\n\r\n    public void onEvent(KinesisEvent t, long l, boolean bln) throws Exception {\r\n        String data = t.getData();\r\n        _logger.debug(\"Got an event: {}\", data);\r\n        // data may consist of multiple lines\r\n        StringTokenizer tok = new StringTokenizer(data, \"\\n\");\r\n        while(tok.hasMoreTokens()){\r\n            String line = tok.nextToken();\r\n            _logger.debug(\"Line = {}\", line);\r\n            Matcher m = _pattern.matcher(line);\r\n            if(m.matches()){\r\n                String page = m.group(1);\r\n                _logger.debug(\"Found page {}\", page);\r\n                if(page != null){\r\n                    _counts.add(page);\r\n                }\r\n            }\r\n            else{\r\n                _logger.debug(\"Unmatched log line\");\r\n            }\r\n        }\r\n    }\r\n\r\nApplication configuration is identical to the previous example. We, therefore, don't need to change the configuration file.\r\n\r\nIn order to start the application we may use the same Surf command with changed parameters:\r\n\r\n    ./surf.sh page-count app-node1\r\n\r\n###Configuring Surf Data Source\r\nThe data source for this example is a growing weblog file. Let’s create a new directory _**surf-home**/data_ and copy the test data - _epa-http.txt_ file.  \r\n\r\nNext, let’s create a different data source configuration file for this scenario. File  _source-node2.conf_ should contain the following entries:\r\n\r\n    aws-access-key-id: <your aws access key>\r\n    aws-secret-key: <your aws secret key>\r\n    aws-kinesis-stream-name: <kinesis stream name>\r\n    vds-source-class: com.informatica.binge.sources.file.BingeFileReader\r\n    flight_size: 10\r\n    directory: ../data\r\n    filename:  test_data.txt\r\n\r\nBecause Surf monitors data file for inserted records, we need to take a few steps to enable the record stream:\r\n\r\n    $touch test_data.txt\r\n    $../bin/surf.sh start-node source-node2.conf\r\n    $cat epa-http.txt >> test_data.txt\r\n\r\nNow the Surf data source will transfer a few records at a time to the page-count application.\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}